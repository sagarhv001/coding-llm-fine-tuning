{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y bitsandbytes torch numpy transformers peft datasets accelerate wandb\n!pip install torch transformers peft datasets accelerate bitsandbytes\n!pip install wandb\n!pip install rouge_score\n!pip install evaluate\n!pip install PyDrive2\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-26T17:20:28.704258Z","iopub.execute_input":"2025-02-26T17:20:28.704597Z","iopub.status.idle":"2025-02-26T17:20:34.540166Z","shell.execute_reply.started":"2025-02-26T17:20:28.704568Z","shell.execute_reply":"2025-02-26T17:20:34.539408Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=c67f658ecf6a7acb729c7d0d52d65f961407ca2489140f948d44c5175627647d\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"!git config --global credential.helper store","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:14:09.698087Z","iopub.execute_input":"2025-02-26T08:14:09.698383Z","iopub.status.idle":"2025-02-26T08:14:09.993273Z","shell.execute_reply.started":"2025-02-26T08:14:09.698360Z","shell.execute_reply":"2025-02-26T08:14:09.992326Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!huggingface-cli login --token \"HF_TOKEN\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:14:11.679833Z","iopub.execute_input":"2025-02-26T08:14:11.680159Z","iopub.status.idle":"2025-02-26T08:14:12.545827Z","shell.execute_reply.started":"2025-02-26T08:14:11.680131Z","shell.execute_reply":"2025-02-26T08:14:12.544730Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nThe token `fine-tuning-llama-model` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `fine-tuning-llama-model`\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n\nmodel_name = \"meta-llama/Llama-3.2-1B\"\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Load model with quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",  # Ensures proper precision handling\n    device_map=\"auto\"  # Uses GPU if available\n)\n\n# Check if it works\nprint(\"Model loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:14:14.373927Z","iopub.execute_input":"2025-02-26T08:14:14.374259Z","iopub.status.idle":"2025-02-26T08:15:17.721944Z","shell.execute_reply.started":"2025-02-26T08:14:14.374229Z","shell.execute_reply":"2025-02-26T08:15:17.721278Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"105dfa70d8ad42a0afb47cd1c78166be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43232c0aa9564df98c78dd3cbc95b175"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e66b5fdaf904789971c432c9eeb0b96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f21aa77e9cc0477badb4c2a02a086a97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6ece7a9d6a34306b8c3ae760cc0ff4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45c73890de0a43899f79f169ef74373b"}},"metadata":{}},{"name":"stdout","text":"Model loaded successfully!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=8,  # Rank of decomposition\n    lora_alpha=16,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\", \"v_proj\"]  # Apply LoRA to attention layers\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:16:21.185159Z","iopub.execute_input":"2025-02-26T08:16:21.185500Z","iopub.status.idle":"2025-02-26T08:16:21.233411Z","shell.execute_reply.started":"2025-02-26T08:16:21.185472Z","shell.execute_reply":"2025-02-26T08:16:21.232779Z"}},"outputs":[{"name":"stdout","text":"trainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.0689\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"code-search-net/code_search_net\",\"python\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:16:25.940189Z","iopub.execute_input":"2025-02-26T08:16:25.940499Z","iopub.status.idle":"2025-02-26T08:18:14.283008Z","shell.execute_reply.started":"2025-02-26T08:16:25.940441Z","shell.execute_reply":"2025-02-26T08:18:14.282229Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/12.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d2b112a6b7478eadd55757e374e0c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"code_search_net.py:   0%|          | 0.00/8.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1592d328db2741c8b1da6100cce580ca"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for code-search-net/code_search_net contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/code-search-net/code_search_net.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"python.zip:   0%|          | 0.00/941M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96b95f78cc874058a8e64096fe77132a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dd6a47e514c4368a135586e3ea5a139"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ac3df2a72384f94be6584b2e646365a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d346f2a09df4253b566c35534155550"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"dataset[\"train\"] = dataset[\"train\"].shuffle(seed=42).select(range(50000))\ndataset[\"test\"] = dataset[\"test\"].shuffle(seed=42).select(range(10000))\ndataset[\"validation\"] = dataset[\"validation\"].shuffle(seed=42).select(range(20000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:20:06.383780Z","iopub.execute_input":"2025-02-26T08:20:06.384081Z","iopub.status.idle":"2025-02-26T08:20:06.430918Z","shell.execute_reply.started":"2025-02-26T08:20:06.384056Z","shell.execute_reply":"2025-02-26T08:20:06.430281Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"if tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token  # Use the EOS token as the padding token\n\n# Tokenization function\ndef tokenize_function(example):\n    tokenized = tokenizer(example[\"func_code_string\"], truncation=True, padding=\"max_length\", max_length=512)\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # Set labels for loss computation\n    return tokenized\n\n# Tokenize the dataset\ntokenized_datasets = dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:20:10.366430Z","iopub.execute_input":"2025-02-26T08:20:10.366748Z","iopub.status.idle":"2025-02-26T08:21:04.744640Z","shell.execute_reply.started":"2025-02-26T08:20:10.366724Z","shell.execute_reply":"2025-02-26T08:21:04.743891Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f489f62ef95426597d644a02d21d377"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae9caa0daa46452d8552b7a45a15ccd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5580dd5ed1814dcbb135460079978300"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"columns_to_keep = [\"input_ids\", \"attention_mask\"]\ntokenized_datasets = tokenized_datasets.remove_columns(\n    [col for col in tokenized_datasets[\"train\"].column_names if col not in columns_to_keep]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:24:54.683915Z","iopub.execute_input":"2025-02-26T08:24:54.684267Z","iopub.status.idle":"2025-02-26T08:24:54.694721Z","shell.execute_reply.started":"2025-02-26T08:24:54.684228Z","shell.execute_reply":"2025-02-26T08:24:54.693978Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import wandb\n\n# Replace with your actual W&B API key\nWANDB_API_KEY = \"your_api_token\"\n\n# Log in to W&B\nwandb.login(key=WANDB_API_KEY)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:21:42.872102Z","iopub.execute_input":"2025-02-26T08:21:42.872441Z","iopub.status.idle":"2025-02-26T08:21:51.816771Z","shell.execute_reply.started":"2025-02-26T08:21:42.872411Z","shell.execute_reply":"2025-02-26T08:21:51.816071Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msagarhv001\u001b[0m (\u001b[33msagarhv001-sagarhv001\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)  # Set `mlm=True` for masked LM\n\n# Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./llama-codesearchnet\", # Save directory\n    run_name=\"llama-codesearchnet-finetune\",  # Set a custom run name\n    per_device_train_batch_size=2,  # Reduce batch size to fit in GPU\n    gradient_accumulation_steps=4,  # Accumulate gradients for stability\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    fp16=True,  # Enable mixed precision for better performance\n    push_to_hub=False, # Disable Hugging Face Hub push\n    remove_unused_columns=False\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:25:22.004687Z","iopub.execute_input":"2025-02-26T08:25:22.004984Z","iopub.status.idle":"2025-02-26T08:25:22.045973Z","shell.execute_reply.started":"2025-02-26T08:25:22.004961Z","shell.execute_reply":"2025-02-26T08:25:22.045294Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-15-d3a8d08bd71c>:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:25:26.013040Z","iopub.execute_input":"2025-02-26T08:25:26.013368Z","iopub.status.idle":"2025-02-26T16:27:07.571946Z","shell.execute_reply.started":"2025-02-26T08:25:26.013338Z","shell.execute_reply":"2025-02-26T16:27:07.571093Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250226_082526-9q41jmpy</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sagarhv001-sagarhv001/huggingface/runs/9q41jmpy' target=\"_blank\">llama-codesearchnet-finetune</a></strong> to <a href='https://wandb.ai/sagarhv001-sagarhv001/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sagarhv001-sagarhv001/huggingface' target=\"_blank\">https://wandb.ai/sagarhv001-sagarhv001/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sagarhv001-sagarhv001/huggingface/runs/9q41jmpy' target=\"_blank\">https://wandb.ai/sagarhv001-sagarhv001/huggingface/runs/9q41jmpy</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [18750/18750 8:01:30, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>7.340700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>7.137900</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>7.159500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>7.120200</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>7.084700</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>7.091400</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>7.077800</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>7.123400</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>7.047900</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>7.077400</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>7.058400</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>7.064400</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>7.111700</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>7.031600</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>6.993600</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>7.019800</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>7.040100</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>7.113700</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>7.043900</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>7.054300</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>7.030100</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>7.107100</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>7.034100</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>7.036000</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>7.046100</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>7.034300</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>7.020100</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>7.024300</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>7.052800</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>7.035700</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>7.033600</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>7.047000</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>7.023100</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>7.000300</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>7.011500</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>7.064900</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>7.036800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=18750, training_loss=7.064436282552084, metrics={'train_runtime': 28901.1705, 'train_samples_per_second': 5.19, 'train_steps_per_second': 0.649, 'total_flos': 4.48818315264e+17, 'train_loss': 7.064436282552084, 'epoch': 3.0})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"metrics = trainer.evaluate()\nprint(metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T16:27:18.345363Z","iopub.execute_input":"2025-02-26T16:27:18.345707Z","iopub.status.idle":"2025-02-26T16:57:36.588893Z","shell.execute_reply.started":"2025-02-26T16:27:18.345681Z","shell.execute_reply":"2025-02-26T16:57:36.588127Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2502' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2500/2500 53:31]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_runtime': 1818.2336, 'eval_samples_per_second': 11.0, 'eval_steps_per_second': 1.375, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"test_code = \"Write a Python code to implement a function to take a list input of numbers as int and str, i.e [1,2,3,'4',5] and change it to ['1','2','3',4,'5']\"\n\ninputs = tokenizer(test_code, return_tensors=\"pt\").to(\"cuda\")\noutput = model.generate(**inputs, max_length=100)\ngenerated_code = tokenizer.decode(output[0], skip_special_tokens=True)\n\nprint(\"Generated Code:\\n\", generated_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T17:02:51.266115Z","iopub.execute_input":"2025-02-26T17:02:51.266398Z","iopub.status.idle":"2025-02-26T17:02:53.297421Z","shell.execute_reply.started":"2025-02-26T17:02:51.266376Z","shell.execute_reply":"2025-02-26T17:02:53.296738Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Code:\n Write a Python code to implement a function to take a list input of numbers as int and str, i.e [1,2,3,'4',5] and change it to ['1','2','3',4,'5'].\n```\ndef change_list_to_str(my_list):\n    new_list = []\n    for item in my_list:\n        if type(item) is int:\n            new_list.append(str(item))\n        else:\n            new_list.append(item)\n    return new_list\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import json\nimport torch\n\n# Save model\nmodel.save_pretrained(\"/kaggle/working/llama_finetuned_model\")\ntokenizer.save_pretrained(\"/kaggle/working/llama_finetuned_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T17:07:33.720382Z","iopub.execute_input":"2025-02-26T17:07:33.720709Z","iopub.status.idle":"2025-02-26T17:07:33.993737Z","shell.execute_reply.started":"2025-02-26T17:07:33.720687Z","shell.execute_reply":"2025-02-26T17:07:33.993004Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/llama_finetuned_model/tokenizer_config.json',\n '/kaggle/working/llama_finetuned_model/special_tokens_map.json',\n '/kaggle/working/llama_finetuned_model/tokenizer.json')"},"metadata":{}}],"execution_count":30}]}